\contentsline {chapter}{\numberline {1}Probability and Inference}{4}{chapter.1}
\contentsline {section}{\numberline {1.1}Introduction to Probability}{4}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Introduction}{4}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}A First Look at Probability}{4}{subsection.1.1.2}
\contentsline {paragraph}{Simulating Coin Flips}{4}{section*.2}
\contentsline {paragraph}{Computer note:}{6}{section*.3}
\contentsline {subsection}{\numberline {1.1.3}Probability and the Art of Modeling Uncertainty}{7}{subsection.1.1.3}
\contentsline {section}{\numberline {1.2}Probability Spaces and Events}{7}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Two Ingredients to Modeling Uncertainty}{7}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Probability Spaces}{8}{subsection.1.2.2}
\contentsline {paragraph}{Notation:}{8}{section*.4}
\contentsline {paragraph}{Why finite?}{8}{section*.5}
\contentsline {subsection}{\numberline {1.2.3}Table Representation}{9}{subsection.1.2.3}
\contentsline {subsection}{\numberline {1.2.4}More on Sample Spaces}{9}{subsection.1.2.4}
\contentsline {subsection}{\numberline {1.2.5}Probabilities with Events}{9}{subsection.1.2.5}
\contentsline {subsection}{\numberline {1.2.6}Events as Sets}{10}{subsection.1.2.6}
\contentsline {subsection}{\numberline {1.2.7}Code for Dealing with Sets in Python}{10}{subsection.1.2.7}
\contentsline {subsection}{\numberline {1.2.8}Probabilities with Events and Code}{10}{subsection.1.2.8}
\contentsline {section}{\numberline {1.3}Random Variables}{10}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}A First Look at Random Variables}{10}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Random Variables}{11}{subsection.1.3.2}
\contentsline {paragraph}{Definition of a ``finite random variable'' (in this course, we will just call this a ``random variable''):}{12}{section*.6}
\contentsline {paragraph}{Quick summary:}{12}{section*.7}
\contentsline {paragraph}{Explanation using a picture:}{12}{section*.8}
\contentsline {paragraph}{Technical note:}{12}{section*.9}
\contentsline {subsection}{\numberline {1.3.3}Two Ways to Specify a Random Variable in Code}{13}{subsection.1.3.3}
\contentsline {paragraph}{Approach 1.}{13}{section*.10}
\contentsline {paragraph}{Approach 2.}{13}{section*.11}
\contentsline {subsection}{\numberline {1.3.4}Random Variables Notation and Terminology}{13}{subsection.1.3.4}
\contentsline {section}{\numberline {1.4}Jointly Distributed Random Variables}{14}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Relating Two Random Variables}{14}{subsection.1.4.1}
\contentsline {paragraph}{Conceptual note:}{15}{section*.12}
\contentsline {paragraph}{Preview of inference:}{15}{section*.13}
\contentsline {subsection}{\numberline {1.4.2}Representing a Joint Probability Table in Code}{15}{subsection.1.4.2}
\contentsline {paragraph}{Approach 0:}{15}{section*.14}
\contentsline {paragraph}{Approach 1:}{16}{section*.15}
\contentsline {paragraph}{Approach 2:}{16}{section*.16}
\contentsline {paragraph}{Some remarks:}{17}{section*.17}
\contentsline {subsection}{\numberline {1.4.3}Marginalization}{18}{subsection.1.4.3}
\contentsline {paragraph}{Marginalization:}{19}{section*.18}
\contentsline {subsection}{\numberline {1.4.4}Marginalization for Many Random Variables}{19}{subsection.1.4.4}
\contentsline {subsection}{\numberline {1.4.5}Conditioning for Random Variables}{20}{subsection.1.4.5}
\contentsline {paragraph}{Notation:}{21}{section*.19}
\contentsline {paragraph}{Conditioning:}{21}{section*.20}
\contentsline {paragraph}{Computational interpretation:}{21}{section*.21}
\contentsline {subsection}{\numberline {1.4.6}Moving Toward a More General Story for Conditioning}{21}{subsection.1.4.6}
\contentsline {section}{\numberline {1.5}Conditioning on Events}{22}{section.1.5}
\contentsline {subsection}{\numberline {1.5.1}Conditioning on Events Intro}{22}{subsection.1.5.1}
\contentsline {subsection}{\numberline {1.5.2}The Product Rule for Events}{22}{subsection.1.5.2}
\contentsline {subsection}{\numberline {1.5.3}Bayes' Theorem for Events}{22}{subsection.1.5.3}
\contentsline {paragraph}{Important note about dividing by probabilities:}{22}{section*.22}
\contentsline {subsection}{\numberline {1.5.4}Practice Problem: Bayes' Theorem and Total Probability}{23}{subsection.1.5.4}
\contentsline {paragraph}{Solution:}{23}{section*.23}
\contentsline {paragraph}{Alternate method to compute the denominator $\mathbb {P}(B_ i^ c)$:}{23}{section*.24}
\contentsline {paragraph}{Take-Away Lessons:}{24}{section*.25}
\contentsline {section}{\numberline {1.6}Inference with Bayes' Theorem for Random Variables}{24}{section.1.6}
\contentsline {subsection}{\numberline {1.6.1}Moving Toward Bayes' Theorem for Random Variables}{24}{subsection.1.6.1}
\contentsline {subsection}{\numberline {1.6.2}The Product Rule for Random Variables (Also Called the Chain Rule)}{24}{subsection.1.6.2}
\contentsline {paragraph}{Interpretation:}{25}{section*.26}
\contentsline {paragraph}{Claim:}{25}{section*.27}
\contentsline {paragraph}{Proof:}{25}{section*.28}
\contentsline {paragraph}{Important convention for this course:}{25}{section*.29}
\contentsline {paragraph}{The product rule is symmetric:}{25}{section*.30}
\contentsline {paragraph}{Interpretation:}{26}{section*.31}
\contentsline {paragraph}{Many random variables:}{26}{section*.32}
\contentsline {subsection}{\numberline {1.6.3}Bayes' Rule for Random Variables (Also Called Bayes' Theorem for Random Variables}{26}{subsection.1.6.3}
\contentsline {paragraph}{Bayes' theorem:}{27}{section*.33}
\contentsline {paragraph}{Important:}{27}{section*.34}
\contentsline {paragraph}{Proof:}{27}{section*.35}
\contentsline {subsection}{\numberline {1.6.4}Bayes' Theorem for Random Variables: A Computational View}{27}{subsection.1.6.4}
\contentsline {subsection}{\numberline {1.6.5}Maximum A Posteriori (MAP) Estimation}{28}{subsection.1.6.5}
\contentsline {section}{\numberline {1.7}Independence Structure}{28}{section.1.7}
\contentsline {subsection}{\numberline {1.7.1}Independent Events}{28}{subsection.1.7.1}
\contentsline {subsection}{\numberline {1.7.2}Independent Random Variables}{28}{subsection.1.7.2}
\contentsline {paragraph}{Exercise: Independent Random Variables}{29}{section*.36}
\contentsline {subsection}{\numberline {1.7.3}Mutual vs Pairwise Independence}{29}{subsection.1.7.3}
\contentsline {subsection}{\numberline {1.7.4}Conditional Independence}{31}{subsection.1.7.4}
\contentsline {paragraph}{Part $(a)$:}{31}{section*.37}
\contentsline {paragraph}{Part $(b)$:}{31}{section*.38}
\contentsline {subsection}{\numberline {1.7.5}Explaining Away}{32}{subsection.1.7.5}
\contentsline {subsection}{\numberline {1.7.6}Practice Problem: Conditional Independence}{34}{subsection.1.7.6}
\contentsline {section}{\numberline {1.8}Decisions and Expectations}{35}{section.1.8}
\contentsline {subsection}{\numberline {1.8.1}Introduction to Decision Making and Expectations}{35}{subsection.1.8.1}
\contentsline {subsection}{\numberline {1.8.2}The Expected Value of a Random Variable}{36}{subsection.1.8.2}
\contentsline {paragraph}{Definition of expected value:}{36}{section*.39}
\contentsline {subsection}{\numberline {1.8.3}Variance and Standard Deviation}{37}{subsection.1.8.3}
\contentsline {subsection}{\numberline {1.8.4}Practice Problem: The Law of Total Expectation}{38}{subsection.1.8.4}
\contentsline {paragraph}{Solution:}{38}{section*.40}
\contentsline {section}{\numberline {1.9}Measuring Randomness}{39}{section.1.9}
\contentsline {subsection}{\numberline {1.9.1}Introduction to Information-Theoretic Measures of Randomness}{39}{subsection.1.9.1}
\contentsline {subsection}{\numberline {1.9.2}Shannon Information Content}{39}{subsection.1.9.2}
\contentsline {subsection}{\numberline {1.9.3}Shannon Entropy}{40}{subsection.1.9.3}
\contentsline {paragraph}{Notation:}{41}{section*.41}
\contentsline {subsection}{\numberline {1.9.4}Information Divergence}{41}{subsection.1.9.4}
\contentsline {subsection}{\numberline {1.9.5}Proof of Gibbs' Inequality}{42}{subsection.1.9.5}
\contentsline {paragraph}{Gibbs' inequality:}{43}{section*.42}
\contentsline {paragraph}{Proof:}{43}{section*.43}
\contentsline {paragraph}{Claim:}{44}{section*.44}
\contentsline {paragraph}{Proof:}{44}{section*.45}
\contentsline {subsection}{\numberline {1.9.6}Mutual Information}{44}{subsection.1.9.6}
\contentsline {subsection}{\numberline {1.9.7}Exercise: Mutual Information}{45}{subsection.1.9.7}
\contentsline {subsection}{\numberline {1.9.8}Information-Theoretic Measures of Randomness: Where We'll See Them Next}{46}{subsection.1.9.8}
\contentsline {section}{\numberline {1.10}Towards Infinity in Modeling Uncertainty}{46}{section.1.10}
\contentsline {subsection}{\numberline {1.10.1}Infinite Outcomes}{46}{subsection.1.10.1}
\contentsline {subsection}{\numberline {1.10.2}The Geometric Distribution}{46}{subsection.1.10.2}
\contentsline {subsection}{\numberline {1.10.3}Practice Problem: The Geometric Distribution}{47}{subsection.1.10.3}
\contentsline {paragraph}{Solution: }{47}{section*.46}
\contentsline {paragraph}{Solution: }{48}{section*.47}
\contentsline {subsection}{\numberline {1.10.4}Discrete Probability Spaces and Random Variables}{48}{subsection.1.10.4}
\contentsline {paragraph}{Definition of a ``discrete probability space'':}{48}{section*.48}
\contentsline {paragraph}{Definition of a ``discrete random variable'':}{48}{section*.49}
\contentsline {chapter}{\numberline {2}Inference in Graphical Models}{49}{chapter.2}
\contentsline {section}{\numberline {2.1}Introduction}{49}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Introduction to Inference in Graphical Models}{49}{subsection.2.1.1}
\contentsline {section}{\numberline {2.2}Efficiency in Computer Programs}{49}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Big O Notation}{49}{subsection.2.2.1}
\contentsline {paragraph}{Big O notation:}{50}{section*.50}
\contentsline {paragraph}{Example:}{50}{section*.51}
\contentsline {paragraph}{Example:}{50}{section*.52}
\contentsline {subsection}{\numberline {2.2.2}Big O Notation with Multiple Variables}{51}{subsection.2.2.2}
\contentsline {paragraph}{Example:}{51}{section*.53}
\contentsline {subsection}{\numberline {2.2.3}Important Remarks Regarding Big O Notation}{51}{subsection.2.2.3}
\contentsline {section}{\numberline {2.3}Graphical Models}{52}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Graphical Models}{52}{subsection.2.3.1}
\contentsline {paragraph}{Definition of an undirected pairwise graphical model}{54}{section*.54}
\contentsline {paragraph}{Important:}{54}{section*.55}
\contentsline {subsection}{\numberline {2.3.2}Trees}{54}{subsection.2.3.2}
\contentsline {paragraph}{Theorem:}{55}{section*.56}
\contentsline {paragraph}{Proof:}{55}{section*.57}
\contentsline {subsection}{\numberline {2.3.3}Practice Problem: Computing the Normalization Constant }{55}{subsection.2.3.3}
\contentsline {paragraph}{Solution:}{55}{section*.58}
\contentsline {section}{\numberline {2.4}Inference in Graphical Models - Marginalization}{56}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Two Fundamental Inference Tasks in Graphical Models}{56}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}The Sum-Product Algorithm}{56}{subsection.2.4.2}
\contentsline {subsection}{\numberline {2.4.3}Speeding Up Sum-Product}{60}{subsection.2.4.3}
\contentsline {section}{\numberline {2.5}Special Case: Marginalization in Hidden Markov Models}{61}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Introduction to Hidden Markov Models (HMM's)}{61}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Hidden Markov Models: Three Ingredients}{61}{subsection.2.5.2}
\contentsline {paragraph}{Clarification:}{62}{section*.59}
\contentsline {paragraph}{Technical note:}{64}{section*.60}
\contentsline {subsection}{\numberline {2.5.3}Formulating HMM's}{64}{subsection.2.5.3}
\contentsline {subsection}{\numberline {2.5.4}Forward and Backward Messages for HMM's}{65}{subsection.2.5.4}
\contentsline {section}{\numberline {2.6}Inference with Graphical Models - Most Probable Configuration}{67}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Most Probable Configurations in Graphical Models}{67}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}The Max-Product Algorithm}{67}{subsection.2.6.2}
\contentsline {subsection}{\numberline {2.6.3}Practice Problem: A Case When Traceback Tables Aren't Needed - Each Node Max-Marginal Has a Unique Most Probable Value}{68}{subsection.2.6.3}
\contentsline {paragraph}{Show:}{69}{section*.61}
\contentsline {paragraph}{Practical implication:}{69}{section*.62}
\contentsline {paragraph}{Solution:}{69}{section*.63}
\contentsline {subsection}{\numberline {2.6.4}Numerical Stability Issues: Max-Product to Min-Sum}{69}{subsection.2.6.4}
\contentsline {section}{\numberline {2.7}The Viterbi Algorithm}{70}{section.2.7}
\contentsline {chapter}{\numberline {3}Learning a Probabilistic Model from Data}{75}{chapter.3}
\contentsline {chapter}{\numberline {A}Notation Summary}{76}{appendix.A}
\contentsline {chapter}{\numberline {B}Supplementary Information}{78}{appendix.B}
\contentsline {section}{\numberline {B.1}External Resources}{78}{section.B.1}
\contentsline {section}{\numberline {B.2}Hints from the Discussion Forum}{78}{section.B.2}
\contentsline {subsection}{\numberline {B.2.1}Law of Total Expectation as Matrices}{78}{subsection.B.2.1}
