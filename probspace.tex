\documentclass[6008notes.tex]{subfiles}
\begin{document}
\graphicspath{ {images/probspace/} }

\section{Probability Spaces and Events}

\subsection{Two Ingredients to Modeling Uncertainty}

When we think of an uncertain world, we will always think of there being some underlying experiment of interest. To model this uncertain world, it suffices to keep track of two things:

The set of all possible outcomes for the experiment: this set is called the sample space and is usually denoted by the Greek letter Omega $\Omega$.
(For the fair coin flip, there are exactly two possible outcomes: heads, tails. Thus, $\Omega =\{ \text {heads},\text {tails}\}$.)

The probability of each outcome: for each possible outcome, assign a probability that is at least 0 and at most 1.
(For the fair coin flip, $\mathbb {P}(\text {heads})=\frac{1}{2}$ and $\mathbb {P}(\text {tails})=\frac{1}{2}$.)

\textbf{Notation:} Throughout this course, for any statement $\mathcal{S}$, ``$\mathbb {P}(\mathcal{S})$'' denotes the probability of $\mathcal{S}$ happening.

In Python:

\begin{lstlisting}
> model = {'heads': 1/2, 'tails': 1/2}
\end{lstlisting}
In particular, we see that we can model uncertainty in code using a Python dictionary. The sample space is precisely the keys in the dictionary:

\begin{lstlisting}
> sample_space = set(model.keys())
{'tails', 'heads'}
\end{lstlisting}
Of course, the dictionary gives us the assignment of probabilities, meaning that for each outcome in the sample space (i.e., for each key in the dictionary), we have an assigned probability:

\begin{lstlisting}
> model['heads']
0.5
> model['tails']
0.5
\end{lstlisting}

A few important remarks:
\begin{itemize}
\item The sample space is always specified to be \textit{collectively exhaustive}, meaning that every possible outcome is in it, and \textit{mutually exclusive}, meaning that once the experiment is run (e.g., flipping the fair coin), exactly one possible outcome in the sample space happens. It's impossible for multiple outcomes in the sample space to simultaneously happen! It's also impossible for none of the outcomes to happen!

\item Probabilities can be thought of as fractions of times outcomes occur; thus, probabilities are nonnegative and at least 0 and at most 1.

\item If we add up the probabilities of all the possible outcomes in the sample space, we get 1.
(For the fair coin flip, $\mathbb {P}(\text {heads})+\mathbb {P}(\text {tails})=\frac{1}{2}+\frac{1}{2}=1$.)
\end{itemize}

Some intuition for this: Consider the coin flipping experiment. What does the fraction of times heads occur and the fraction of times tails occur add up to? Since these are the only two possible outcomes (and again, recall that these outcomes are exclusive in that they can't simultaneously occur, and exhaustive since they are the only possible outcomes), these two fractions will always sum to 1. For a massive number of repeats of the experiment, these two fractions correspond to $\mathbb {P}(\text {heads})$ and $\mathbb {P}(\text {tails})$; the fractions sum to 1 and so these probabilities also sum to 1.

\subsection{Probability Spaces}

At this point, we've actually already seen the most basic data structure used throughout this course for modeling uncertainty, called a \textit{finite probability space} (in this course, we'll often also just call this either a \textit{probability space} or a \textit{probability model}):

A \textit{finite probability space} consists of two ingredients:
\begin{itemize}
 \item a sample space $\Omega$ consisting of a \textit{finite} (i.e., not infinite) number of collectively exhaustive and mutually exclusive possible outcomes

\item an assignment of probabilities: for each possible outcome $\omega \in \Omega$, we assign a probability $\mathbb {P}(\text {outcome }\omega )$ at least 0 and at most 1, where we require that the probabilities across all the possible outcomes in the sample space add up to 1:\\
$\sum _{\omega \in \Omega }\mathbb {P}(\text {outcome }\omega )=1$
\end{itemize}

 
\paragraph{Notation:} As shorthand we occasionally use the tuple ``$(\Omega ,\mathbb {P})$'' to refer to a finite probability space to remind ourselves of the two ingredients needed, sample space $\Omega$ and an assignment of probabilities $\mathbb {P}$. As we already saw, in code these two pieces can be represented together in a single Python dictionary. However, when we want to reason about probability spaces in terms of the mathematics, it's helpful to have names for the two pieces.

\paragraph{Why finite?} Of the two pieces making up a finite probability space $(\Omega ,\mathbb {P})$, the sample space $\Omega$ being finite is a fairly natural constraint, corresponding to how we typically work with Python dictionaries where there is only a finite number of keys. As we'll see, finite probability spaces are already extremely useful in practice. Pedagogically, finite probability spaces also provide a great intro to probability theory as they already carry a wealth of intuition, much of which carries over to a more complete story of general probability spaces!

\subsection{Table Representation}

A probability space is a data structure in that we can always visualize as a table of nonnegative entries that sum to 1. Let's see a concrete example of this, first writing the table out on paper and then coding it up.

\textbf{Example:} Suppose we have a model of tomorrow's weather given as follows: sunny with probability 1/2, rainy with probability 1/6, and snowy with probability 1/3. Here's the probability space, shown as a table:

\begin{center}
\begin{tabular}{c c c}
 & & \textbf{Probability} \\
 &	sunny	& 1/2 \\
\textbf{Outcome} &	rainy &	1/6 \\
 &	snowy	& 1/3
\end{tabular}
\end{center}

Note: This a table of 3 nonnegative entries that sum to 1. The rows correspond to the sample space\\
 $\Omega =\{ \text {sunny},\text {rainy},\text {snowy}\}$.

We will often use this table representation of a probability space to tell you how we're modeling uncertainty for a particular problem. It provides the simplest of visualizations of a probability space.

Of course, in Python code, the above probability space is given by:

\begin{lstlisting}
prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}
\end{lstlisting}
A different way to code up the same probability space is to separately specify the outcomes (i.e., the sample space) and the probabilities:

\begin{lstlisting}
outcomes = ['sunny', 'rainy', 'snowy']
probabilities = np.array([1/2, 1/6, 1/3])
\end{lstlisting}
The i-th entry of \texttt{outcomes} has probability given by the i-th entry of \texttt{probabilities}. Note that \texttt{probabilities} is a vector of numbers that we represent as a Numpy array. Numpy has various built-in methods that enable us to easily work with vectors (and more generally arrays) of numbers.

\subsection{More on Sample Spaces}

In the video, we saw that a sample space encoding the outcomes of 2 coin flips encodes all the information for 1 coin flip as well. Thus, we could use the same sample space to model a single coin flip. However, if we really only cared about a single coin flip, then a sample space encoding 2 coin flips is richer than we actually need it to be!

When we model some uncertain situation, how we specify a sample space is not unique. We saw an example of this already in an earlier exercise where for rolling a single six-sided die, we can choose to name the outcomes differently, saying for instance "roll 1" instead of "1". We could even add a bunch of extraneous outcomes that all have probability 0. We could add extraneous information that doesn't matter such as "Alice rolls 1", "Bob rolls 1", etc where we enumerate out all the people who could roll the die in which the outcome is a 1. Sure, depending on the problem we are trying to solve, maybe knowing who rolled the die is important, but if we don't care about who rolled the die, then the information isn't helpful but it's still possible to include this information in the sample space.

Generally speaking it's best to choose a sample space that is as simple as possible for modeling what we care about solving. For example, if we were rolling a six-sided die, and we actually only care about whether the face shows up at least 4 or not, then it's sufficient to just keep track of two outcomes, "at least 4" and "less than 4".

\subsection{Probabilities with Events}

TODO -- add notes from video

%Now, let's continue with our two-coin flip example. We have defined three events. Each event is a subset of outcomes. Here, event A is, in language, that the second coin flip is a tail. And we can write exactly the same idea mathematically to say this-- A is a subset. It's a set of these two outcomes. Their second coin clip is a tail. Same thing for event B. You say at least one coin flip is a head. That's in language. And the mathematical language of that is to write out this collection of outcomes. Event C was a single outcome. Now, we say-- this is a definition-- we say that an event A occurs if when we run this random experiment, we see an outcome omega. And that happens to be in this set A. Example, if we see tail-tail, then we say event A occurs. We see head-tail, we also say event A occurs. Well, this is important point. At each time you run a random experiment, there can be only one outcome. But there could be multiple events happened at the same time. For example, if we see this head-tail as our outcome, then it is one element in event A. So A occurs. The second coin flip is a tail. And at the same time, event B also occurs. There is at least one head in your outcome. So multiple events can occur at the same time. Now, we also can define the notion of the probability of an event. What is the probability you see an event occur? Probability you see this event A occur. Example. Well, then there must be that the outcome is one of its elements. So we write this as the sum over all outcomes in this set, in this event. Sum up its corresponding probabilities. And this sum of probability is the probability that this event A occurs. In this example, probability of event A is equal to 1/4 for this head-tail plus another 1/4, which is tail-tail. It's equal to 1/2. That's an example of probability of an event.


\subsection{Events as Sets}

TODO -- add notes from video

%We have just defined an event as a subset of outcomes, and event A as a subset of Omega, the overall set of all possible outcomes. This is, in fact, a very convenient way to define events, because we can use set operations to come up with a lot more events. For example, in the previous example, you can talk about A intersect B. That in language is, we want to look at all the outcomes that has the second coin flip as a tail, and at the same time there has to be at least one head. In this case, it's just single element set. It has to be, first it's a head, there has to be at least one head, and the second one is tail. We can talk about union of two sets. For example, A union C. That means we need to have either the second coin flip is a tail, or both coin flips are head. So this is head head, H T, T T. So this defines a more complex event. Or, we can define a set's complement. B complement, that means event B didn't happen. There is not at least a head in the two coin flips. In this case it has to be T T, a single element. There are two special events in every model. One of them is omega itself, the whole set. That includes all possible events. So whatever is the outcome, this event always occurs. And the compliment of that is phi. This is what we use to identify the empty set. No outcome is counted in this. There are some nice properties of the probability of these events. Now let's list some of them-- properties. Number one, we will always have probability of the whole set make it equal to 1. Probability of some outcome happens that's a sure thing. That's a probability of 1. For that reason, the probability of the empty set, we always define that as 0. A second property is, if I have one event included in a different event, this event A is a subset of B, then I always have the probability of A is less than or equal to the probability of B. Because in the definition, the probability of this event is the sum of the probability of all elements in it. And it has more elements, and each one has a non-negative probability with it, so the sum is always larger. This requires that every probability of every event, because A is always a subset of Omega, this has to be always less than or equal to 1. That's from this definition. A third one that's also important is if I have two events that they do not overlap, then their intersection is empty set. Then this would imply, if I look at the probability of the union of the two, I want to include all the outcomes in either A or B, but there is no overlap, then this is equal to probability of A, plus probability of B. A simple consequence of this is because A and A complement always do not overlap, and the union of two Omega-- so we can write, the probability of A compliment is always equal to one, that's just the probability of Omega, the whole set, minus the probability of A. So here are some simple properties of probability. In fact, the probability of event is so convenient that later on when we write probability of a omega, of one element, we actually think of that as probability of a single element set. Think of that as this event that has only one single element. So in this way we can say probabilities are only defined for events. That's a better way to write probabilities.


\subsection{Code for Dealing with Sets in Python}

In the video, the set operations can actually be implemented in Python as follows:

\begin{lstlisting}
sample_space = {'HH', 'HT', 'TH', 'TT'}
A = {'HT', 'TT'}
B = {'HH', 'HT', 'TH'}
C = {'HH'}
A_intersect_B = A.intersection(B)  # equivalent to "B.intersection(A)" or "A & B"
A_union_C = A.union(C)  # equivalent to "C.union(A)" and also "A | C"
B_complement = sample_space.difference(B)  # equivalent also to "sample_space - B"
\end{lstlisting}

\subsection{Probabilities with Events and Code}

From the videos, we see that an event is a subset of the sample space $\Omega$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!

The \textit{probability of an event} $\mathcal{A}\subseteq \Omega$ is the sum of the probabilities of the possible outcomes in $\mathcal{A}$:

$\mathbb {P}(\mathcal{A})\triangleq \sum _{\omega \in \mathcal{A}}\mathbb {P}(\text {outcome }\omega )$,
 
where ``$\triangleq$'' means ``defined as''.

We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set $\texttt{event}$, where the probability space is encoded as a Python dictionary $\texttt{prob\_space}$:

\begin{lstlisting}
def prob_of_event(event, prob_space):
    total = 0
    for outcome in event:
        total += prob_space[outcome]
    return total
\end{lstlisting}

Here's an example of how to use the above function:

\begin{lstlisting}
prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}
rainy_or_snowy_event = {'rainy', 'snowy'}
print(prob_of_event(rainy_or_snowy_event, prob_space))
\end{lstlisting}

\end{document}