\begin{document}
\graphicspath{ {images/slt/} }

\section{Structure Learning -- Trees}

\subsection{Structure Learning for an Undirected Tree-Structured Graphical Model: The Chow-Liu Algorithm}

At this point we've covered parameter learning for undirected trees, where we assume we know the tree structure. But what if we don't know what tree to even use? We now look at an algorithm called the Chow-Liu algorithm that learns which tree to use from training data, again using maximum likelihood. Once more, information measures appear, where mutual information plays a pivotal role. Recall that mutual information tells us how far two random variables are from being independent. A key idea here is that to determine whether an edge between two random variables should be present in a graphical model, we can look at mutual information.

TODO - add notes from video

% Now, I'm going to talk about the structure learning with undirected tree-structured graphical models. And so here, the idea is that we have random variables X 1 up through X k. And so these you can think of as nodes in a undirected graphical model. But we don't know which edges should be in this graphical model. And restricting ourself to trees, the question is, well, which tree should we use? And to help us answer this question, we're going to have access to training data. So we see n i.i.d. samples from this graphical model, where each of the samples, of course, we see everything, x 1 up through x k superscript i. So this is the i-th training data point. Just as a comment, the number of possible trees here is humongous. It's actually k to the k minus 2. This from a result called Cayley's formula. This thing grows even faster than exponential. It's super exponential. So somehow, we're going to search this huge space of possible trees to select the best tree. And it turns out, doing that can be done very quickly. And so how we're going to select the tree is we're going to use maximum likelihood. We're going to pick the tree T here. In this case, we know that there are k nodes. And the question is just, where are the edges? So you can think of T here as just specifying which edges are present that result in a tree across k nodes. So of course, there should be k minus 1 edges in this tree. So we're trying to find the best tree such that once we fix a tree, then we can do maximum likelihood for that specific tree. So that's what's in these curly braces is a parameter estimation. We're going to pick the best parameters for this specific tree. And at least for what comes out here, the arg max, we're only doing the argument over which tree and not over which parameters. Of course, it's easy to figure out which parameters are the best in terms of maximum likelihood for a specific tree, because once you know the tree, you just do the parameter learning approach we saw previously, where we know the tree structure, to get out what the parameters are. And so now, we're going to look at how to solve this optimization problem right here. And it turns out there's a very clean algorithm for solving this. And to get to this algorithm, we're going to simplify what's in the curly braces. So let's look at for a specific tree T what's in the curly braces here. So we have the log likelihood across the training data. Here, we're parameterizing by both what the tree is and then, once we know the tree, some set of parameters dependent on the tree-- so theta sub T for the tree. So again, this is just what's in the curly braces. Well, this thing by the tree factorization, we can arbitrarily choose a root p of, let's call the root, r. And again, I'm going to drop the subscripts here just to save some writing. So p of x r of i-- I'm writing out the factorization-- times the product from j-- so all the nodes that are not equal to the root-- we're going to have p of x j conditioned on the parent of x j, which I'll denote as pi of j, parent of j, i. And again, being informal here, I'm not writing the parameters. We'll see shortly how to deal with that. This thing and then product i going from 1 to n, and then this log. We have this. And this thing is going to split up into the-- we have the log likelihood for the root node. And then we have the log likelihood for each of these conditional probability tables. So here, the key idea is going to be as follows. I've been simplifying what is here. But we want to take the maximum over all the theta T's, as in we want to actually plug in the best possible values of the parameters into this expression. But we know from the parameter learning video that the best choice of theta is actually going to correspond to the empirical distributions. And so if I want the max and not just what was here before doing the max, then all I need to do is replace these probability distributions with the empirical versions. And that would actually give you the max. Good. So that gives us this expression down here. And just as a reminder, p empirical distribution of x r of i, really that's the empirical distribution of random variable X r. Well, this thing evaluated at some value a is just equal to the fraction of times a appears in the training data. So this is going to be 1/n divided by i going from 1 to n of the number of times we see a for the r-th random variable corresponding to the root. So this is just an example of how you get these empirical distributions. And so for the empirical conditional probability table, as a reminder, it's going to be the fraction of times we see that these two random variables in our training data they jointly take on two specific values divided by the number of times we see this random variable taking on a specific value in our training data. So that's what these p hats are. So the next thing we're going to do, we've actually already been doing some of, which is we're going to introduce a different summation to split up what the possible values of this random variable are. We were doing this for the conditional distributions earlier. But now, we're going to do it for all of them. And we're going to count up how many times we see each particular value. So here, sum over a of sum over i going from 1 to n of whether training data point i for the r-th random variable is equal to a... log of p hat of a for X r. So that's what this first thing is equal to. And plus-- for this second thing, we're going to have j not equal to r of sum over all the possible values of a and all the possible values of b. And then now, we're going to sum over the training data of whether the i-th training data point looking at the j-th random variable is equal to a and for the i-th training data point the parent of the j-th random variable is equal to b. So we're going to have this times log of p hat a given b, where this is X j given X pi j. So we're going to write this out in this way, because there's going to be a clean information theoretic way to reason about this. So here, this thing is, of course, going to be n times the empirical distribution for X r evaluated at a. And this thing right here is going to be n times the empirical distribution of X j and X pi j evaluated at a and b. Let me rewrite this line to make it a little cleaner what's going on. We have sum over a of-- I'm going to factor out this n that's appearing in both of these terms-- so n out here, sum over a of p hat X r of a, log p hat X r of a plus sum over j not equal to the root, sum over a, b of-- so I factored out the n-- and then I have p hat X j X pi j, a b, log p hat-- and now, this p hat X j given X pi j, I'm going to split this up. So you can actually use the definition of conditional probability even for an empirical distribution, because an empirical distribution is actually still a distribution. So we can write this piece as p hat of X j, X pi j of a comma b divided by-- and then since we're conditioning on X pi j, it's going to be p hat of X pi j evaluated at b. So we're going to have this. And note that what I'm going to do is first move this over a little bit. I'm going to multiply top and bottom by p hat of X j of a. So down here, it's going to be p hat of X j of a. So I'm just multiplying by 1 here. But if I do that, something's going to happen. So you probably noticed that this thing here is related to the negative entropy. Again, here, the log is natural log. But that's OK. We can still talk about the entropy. It's just not in terms of bits, in terms of "nats". So here, we have entropy of p hat of X r. And then here, we're going to have this thing in blue. So we're going to have this times log of this thing in blue plus this times log of just this piece here. And what that's going to look like is we're going to have sum j not equal to r of-- so summing over a, b of this thing times log of this blue thing, if you at that, you'll notice that that is a information divergence. It is the information divergence between the empirical distribution of X j and X pi j with the empirical distribution of X j product with the empirical distribution for X pi j. The basic idea is we're looking at how far the empirical joint distribution is from a product of two empirical marginal distributions. You can think of this as saying, how far is this empirical joint distribution from being independent. Well, this thing, we actually previously saw. This thing is called the mutual information between these two random variables. But it's not actually these two random variables. It's the empirical versions of them. So we'll actually denote this as the empirical mutual information between X j and X pi j. Again, the mutual information would be this exact same thing except with the true distributions, rather than the empirical distributions. Now, when we're looking at the empirical distributions, we just get this empirical mutual information instead. So that accounts for this blue piece here. And then we have this other piece. So we have this plus sum-- so we're out here again-- sum a and b, pi hat X j, X pi j of a, b times log of-- now just the red guy-- so p hat of X j of a. And note that we can actually-- we have a summation over a and a summation over b-- we can actually sum over b first. And if we do that, if we sum out over b, we'll just get a. And this thing will just become the marginal over just the parts that depend on a. So again, an empirical distribution is still a distribution. So in particular, an empirical joint distribution, if we marginalize out b, we're just going to get what remains for a, which is the empirical marginal distribution for X j evaluated at a. So of course, this thing here is minus entropy of the empirical distribution of X j. Once we put all these pieces together, we have that this whole thing is equal to n times-- so we had this minus entropy of the root node. And then for all the other nodes, there's also a minus entropy for those nodes. So actually, we're going to have a sum over all the nodes j-- so j within and all the nodes-- V of minus entropy of the empirical distribution of X j. So that covers this thing and this thing. And then we're going to have for everything that's not the root node, we're going to have the empirical mutual information between X j-- this should be a semicolon here-- so X j semicolon X pi j. Again, from our mutual information notation, the mutual information between two random variables, we put a semicolon in between. So here, a different way to write this that is cleaner and doesn't depend on us choosing a root is to write this as for i, j within the edges in the tree of-- here, I'm going to write X i, X j. So mutual information is actually symmetric in that the mutual information of X j X i is the same thing as the mutual information of X i X j. And of course, the same is true when we look at the empirical version of it. So bottom line is we get this. So this is equal to the maximum likelihood across parameters for a specific tree T. And so from looking at this, note that to choose the best possible tree-- so to go from this to picking the arg max of which tree, what it amounts to is-- the nodes aren't going to change, so this term is going to be the same across any tree. The only thing that changes is this thing right here that depends on the edges in the tree. So to figure out what the best tree is possible we just want to maximize this second term. I'm now going to present an algorithm that maximizes this log likelihood across all possible trees and the basic idea of the algorithm is going to be that we start with a graph with no edges and we're going to incrementally add edges until we get the best tree according to maximum likelihood and how we're going to choose which edge to add is-- so for example the very first edge we add we're going to look at across all pairs of random variables which one has the highest empirical mutual information, and we're going to add that edge. And then we're going to repeat that process-- we're going to find the pair of random variables with the second-highest empirical mutual information and so forth where we make sure that whenever we add an edge we don't introduce a cycle. And after we add enough edges we're going to get a tree and that's actually going to be the best tree. This resulting algorithm is called the Chow-Liu algorithm which I will now illustrate with a simple four-node example. So here we have K equals 4 for this 4 node example X 1 up through X 4, and in light gray I've drawn out all the edges you could possibly have. And for each of these edges I have a gray number. These gray numbers are the empirical mutual information quantities. Here I'm just making up what these numbers are. In practice of course what you would do is first compute out what all these gray numbers are-- the empirical mutual information quantities-- based on your training data. So here I assume that we've already done that step and we have those empirical mutual information quantities. And once we have that what the Chow-Liu algorithm is going to say is that we're going to look at, well, what is the highest empirical mutual information? It's this 10 so the very first edge we're going to add is this edge right here. And then it says "all right, let's look at the second highest guy," which is this 8, and it will say "okay, add this edge". And then in the next step it's going to say "all right, let's look at the third highest one," which is this 6 here and well if I add this edge I'm actually going to form a cycle. And so I'm not going to add that. Instead I'm going to skip that and move on to the next highest, which is this 4, and then I'll say okay let's add this edge. And at this point the algorithm would actually stop because if it tried to add anything else it would form a cycle. So at this point the Chow-Liu algorithm would output this tree, denoted by these solid black lines, and that would be the maximum likelihood estimate for the best possible tree we could use. So the general idea is very simple. We start with a graph with no edges. And then for all pairs of random variables we compute the empirical mutual information quantities-- those are these gray numbers here. And so of course here i goes from 1 up to k and j also goes from 1 up to k where we don't look at i, i-- okay, i and j are always different. So the next thing we do is we sort these empirical mutual information quantities from highest to lowest. And then starting from the highest and going all the way to the lowest we're going to add edges where again we skip adding an edge if it results in a cycle. And this algorithm-- the Chow-Liu algorithm-- will always terminate because if you remember for a graph with k nodes, any tree for that graph is going to have k minus 1 edges. So for example in this case, any tree for this graph has 3 edges. All right, so that is the Chow-Liu algorithm. Given training data we can learn what is the maximum likelihood estimate for the tree that best explains the data. Be sure to look at the notes for a justification for why the Chow-Liu algorithm is correct.

\end{document}
